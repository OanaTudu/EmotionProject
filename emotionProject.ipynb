{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotion Recognition Project\n",
    "\n",
    "2019 - OT.\n",
    "\n",
    "Given the Karolinska faces collection, train a classifier to predict emotion.\n",
    "Details about the data (KDEF):\n",
    "70 subjects (35 male, 35 female), all white, between 20 and 30 years of age, photographed with 7 emotion expressions:\n",
    "neutral, happy, angry, afraid, disgusted, sad, surprised\n",
    "\n",
    "Files stored in KDEF folder. Image naming code:\n",
    "Codes:\n",
    "\tExample: AF01ANFL.JPG\n",
    "\t\tLetter 1: Session \n",
    "\t\t\t\t\tA = series one\n",
    "\t\t\t\t\tB = series two\n",
    "\t\tLetter 2: Gender \n",
    "\t\t\t\t\tF = female\n",
    "\t\t\t\t\tM = male\n",
    "\t\tLetter 3 & 4: Identity number\n",
    "\t\t\t\t\t01 - 35\n",
    "\t\tLetter 5 & 6: Expression\n",
    "\t\t\t\t\tAF = afraid\n",
    "\t\t\t\t\tAN = angry\n",
    "\t\t\t\t\tDI = disgusted\n",
    "\t\t\t\t\tHA = happy\n",
    "\t\t\t\t\tNE = neutral\n",
    "\t\t\t\t\tSA = sad\n",
    "\t\t\t\t\tSU = surprised\n",
    "\t\tLetter 7 & 8: Angle\n",
    "\t\t\t\t\tFL = full left profile\n",
    "\t\t\t\t\tHL = half left profile\n",
    "\t\t\t\t\tS = straight\n",
    "\t\t\t\t\tHR = half right profile\n",
    "\t\t\t\t\tFR = full right profile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries needed for a CNN model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator \n",
    "from keras.preprocessing.image import save_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Conv2D, MaxPooling2D \n",
    "from keras.layers import Activation, Dropout, Flatten, Dense \n",
    "from keras import backend as K \n",
    "from keras.preprocessing.image import load_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the data from KDEF and copy samples in a data tree structured into a train set (with 290 images per emotion, focusing right now only on fearful and happy), and a validation set with 60 images per emotion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import shutil\n",
    "\n",
    "#Initialize the number of images in class:\n",
    "k = 0\n",
    "afraid = r\"[A-Z][A-Z][0-9][0-9]AF*.JPG\"\n",
    "happy = r\"[A-Z][A-Z][0-9][0-9]HA*.JPG\"\n",
    "basedir = 'data/train/happy/'\n",
    "# List all files in the female directories with angry expressions using scandir()\n",
    "for i in range(1,36):\n",
    "    if i < 10:\n",
    "        n = '0' + str(i)\n",
    "    else:\n",
    "        n = str(i)   \n",
    "    basepath = 'KDEF/KDEF_and_AKDEF/KDEF/AF' + n + '/'\n",
    "    with os.scandir(basepath) as entries:\n",
    "        for entry in entries:\n",
    "            if entry.is_file():\n",
    "                if fnmatch.fnmatch(entry.name, happy):\n",
    "                    #print(entry.name)\n",
    "                    old_name = os.path.join(basepath, entry.name)\n",
    "                    #print(old_name)\n",
    "                     # Initial new name\n",
    "                    new_name = os.path.join(basedir,entry.name)\n",
    "                    #print(new_name)\n",
    "                    shutil.copy(old_name, new_name)\n",
    "                    \n",
    "                    k += 1\n",
    "                    \n",
    "# List all files in the male directories with angry expressions using scandir()                    \n",
    "for i in range(1,36):\n",
    "    if i < 10:\n",
    "        n = '0' + str(i)\n",
    "    else:\n",
    "        n = str(i)   \n",
    "    basepath = 'KDEF/KDEF_and_AKDEF/KDEF/AM' + n + '/'\n",
    "    with os.scandir(basepath) as entries:\n",
    "        for entry in entries:\n",
    "            if entry.is_file():\n",
    "                if fnmatch.fnmatch(entry.name, happy):\n",
    "                    #print(entry.name)\n",
    "                    old_name = os.path.join(basepath, entry.name)\n",
    "                    #print(old_name)\n",
    "                     # Initial new name\n",
    "                    new_name = os.path.join(basedir,entry.name)\n",
    "                    #print(new_name)\n",
    "                    shutil.copy(old_name, new_name)\n",
    "                    k += 1\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the KDEF images are 562 by 762, so we should assign these dimensions.\n",
    "Image size and input shape:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width = 562\n",
    "img_height = 762"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if K.image_data_format() == 'channels_first': \n",
    "    input_shape = (3, img_width, img_height) \n",
    "else: \n",
    "    input_shape = (img_width, img_height, 3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the train data directory, validation directory, and epochs and batch size for the CNN.\n",
    "There are 290 images per emotion in the train set, 580 in total, and 60 images per emotion in the validation set, for a total of 120 images. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'\n",
    "nb_train_samples =580 \n",
    "nb_validation_samples = 120\n",
    "epochs = 20\n",
    "batch_size = 16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the convolution networks model and add the layers. We'll use an RMS optimizer and \"relu\" activation. The output layer - since it's a binary classification - will have a sigmoid activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() \n",
    "model.add(Conv2D(32, (2, 2), input_shape = input_shape)) \n",
    "model.add(Activation('relu')) \n",
    "model.add(MaxPooling2D(pool_size =(2, 2))) \n",
    "  \n",
    "model.add(Conv2D(32, (2, 2))) \n",
    "model.add(Activation('relu')) \n",
    "model.add(MaxPooling2D(pool_size =(2, 2))) \n",
    "  \n",
    "model.add(Conv2D(64, (2, 2))) \n",
    "model.add(Activation('relu')) \n",
    "model.add(MaxPooling2D(pool_size =(2, 2))) \n",
    "  \n",
    "model.add(Flatten()) \n",
    "model.add(Dense(64)) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dropout(0.5)) \n",
    "model.add(Dense(1)) \n",
    "model.add(Activation('sigmoid')) \n",
    "  \n",
    "model.compile(loss ='binary_crossentropy', \n",
    "                     optimizer ='rmsprop', \n",
    "                   metrics =['accuracy']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the training data with a generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator( \n",
    "    rescale=1. / 255, \n",
    "    shear_range=0.2, \n",
    "    zoom_range=0.2, \n",
    "    horizontal_flip=True) \n",
    "  \n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generators for the training and validation images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 640 images belonging to 2 classes.\n",
      "Found 120 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory( \n",
    "    train_data_dir, \n",
    "    target_size=(img_width, img_height), \n",
    "    batch_size=batch_size, \n",
    "    class_mode='binary') \n",
    "  \n",
    "validation_generator = test_datagen.flow_from_directory( \n",
    "    validation_data_dir, \n",
    "    target_size=(img_width, img_height), \n",
    "    batch_size=batch_size, \n",
    "    class_mode='binary') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit the model and see how it converges within 20 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "36/36 [==============================] - 93s 3s/step - loss: 7.2367 - acc: 0.5365 - val_loss: 7.6865 - val_acc: 0.5179\n",
      "Epoch 2/20\n",
      "36/36 [==============================] - 85s 2s/step - loss: 3.4228 - acc: 0.5260 - val_loss: 0.6909 - val_acc: 0.4904\n",
      "Epoch 3/20\n",
      "36/36 [==============================] - 85s 2s/step - loss: 0.7006 - acc: 0.5451 - val_loss: 0.6926 - val_acc: 0.4808\n",
      "Epoch 4/20\n",
      "36/36 [==============================] - 85s 2s/step - loss: 0.7597 - acc: 0.5278 - val_loss: 0.6889 - val_acc: 0.5192\n",
      "Epoch 5/20\n",
      "36/36 [==============================] - 85s 2s/step - loss: 0.7097 - acc: 0.5469 - val_loss: 0.7036 - val_acc: 0.4423\n",
      "Epoch 6/20\n",
      "36/36 [==============================] - 85s 2s/step - loss: 0.6954 - acc: 0.5573 - val_loss: 0.6895 - val_acc: 0.4519\n",
      "Epoch 7/20\n",
      "36/36 [==============================] - 84s 2s/step - loss: 0.7250 - acc: 0.5625 - val_loss: 0.6768 - val_acc: 0.6923\n",
      "Epoch 8/20\n",
      "36/36 [==============================] - 554s 15s/step - loss: 0.6865 - acc: 0.5955 - val_loss: 0.6691 - val_acc: 0.5192\n",
      "Epoch 9/20\n",
      "36/36 [==============================] - 86s 2s/step - loss: 0.6694 - acc: 0.6302 - val_loss: 0.6449 - val_acc: 0.5804\n",
      "Epoch 10/20\n",
      "36/36 [==============================] - 86s 2s/step - loss: 0.6431 - acc: 0.6319 - val_loss: 1.2385 - val_acc: 0.5096\n",
      "Epoch 11/20\n",
      "36/36 [==============================] - 84s 2s/step - loss: 0.6285 - acc: 0.6441 - val_loss: 0.4945 - val_acc: 0.7788\n",
      "Epoch 12/20\n",
      "36/36 [==============================] - 86s 2s/step - loss: 0.5811 - acc: 0.6979 - val_loss: 0.5352 - val_acc: 0.7308\n",
      "Epoch 13/20\n",
      "36/36 [==============================] - 94s 3s/step - loss: 0.5530 - acc: 0.7257 - val_loss: 0.3980 - val_acc: 0.8365\n",
      "Epoch 14/20\n",
      "36/36 [==============================] - 93s 3s/step - loss: 0.5153 - acc: 0.7396 - val_loss: 0.4249 - val_acc: 0.8462\n",
      "Epoch 15/20\n",
      "36/36 [==============================] - 90s 3s/step - loss: 0.5383 - acc: 0.7326 - val_loss: 0.4499 - val_acc: 0.8173\n",
      "Epoch 16/20\n",
      "36/36 [==============================] - 107s 3s/step - loss: 0.4579 - acc: 0.7882 - val_loss: 0.3574 - val_acc: 0.8365\n",
      "Epoch 17/20\n",
      "36/36 [==============================] - 91s 3s/step - loss: 0.4712 - acc: 0.7847 - val_loss: 0.3320 - val_acc: 0.8482\n",
      "Epoch 18/20\n",
      "36/36 [==============================] - 88s 2s/step - loss: 0.4656 - acc: 0.7986 - val_loss: 0.3521 - val_acc: 0.8077\n",
      "Epoch 19/20\n",
      "36/36 [==============================] - 96s 3s/step - loss: 0.4501 - acc: 0.7622 - val_loss: 0.3548 - val_acc: 0.8462\n",
      "Epoch 20/20\n",
      "36/36 [==============================] - 106s 3s/step - loss: 0.4558 - acc: 0.7917 - val_loss: 0.2973 - val_acc: 0.8462\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb366eb160>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator( \n",
    "    train_generator, \n",
    "    steps_per_epoch=nb_train_samples // batch_size, \n",
    "    epochs=epochs, \n",
    "    validation_data=validation_generator, \n",
    "    validation_steps=nb_validation_samples // batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Not too bad - particularly given the caveats of very few images and all the downsides of using KDEF stimuli. We got an accuracy of 79% , with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save_weights('model_saved.h5') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
